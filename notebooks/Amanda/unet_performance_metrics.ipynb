{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "041a9361",
   "metadata": {},
   "source": [
    "# Assessing network performance\n",
    "\n",
    "**Author:** Amanda M. Thomas  \n",
    "**Goal:** This notebook computes some performance metrics commonly used to assess network performance.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dece813-8bb3-460c-89b4-877d03dc2360",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import seisbench.data as sbd\n",
    "import seisbench.models as sbm\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "import seisbench.generate as sbg\n",
    "import pandas as pd\n",
    "from scipy.signal import find_peaks\n",
    "from obspy import Trace, Stream\n",
    "\n",
    "# Set seeds\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48deb6bc",
   "metadata": {},
   "source": [
    "## 1. Load the U-Net Architecture\n",
    "\n",
    "We need to first load in the network architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92785c0a-2cea-401e-944b-567b0859fa89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: U-Net Building Blocks\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, padding=1):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size, padding=padding),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(out_channels, out_channels, kernel_size, padding=padding),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class UNet1D(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=3, features=[16, 32, 64, 128]):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.downs = nn.ModuleList()  # Encoder blocks (downsampling path)\n",
    "        self.ups = nn.ModuleList()    # Decoder blocks (upsampling path)\n",
    "    \n",
    "        # ----- Encoder: Downsampling Path -----\n",
    "        # Each ConvBlock halves the temporal resolution via pooling (done in forward),\n",
    "        # and increases the number of feature channels.\n",
    "        for feat in features:\n",
    "            self.downs.append(ConvBlock(in_channels, feat))  # ConvBlock: Conv + ReLU + Conv + ReLU\n",
    "            in_channels = feat  # Update in_channels for the next block\n",
    "    \n",
    "        # ----- Bottleneck -----\n",
    "        # Deepest layer in the U-Net, connects encoder and decoder\n",
    "        self.bottleneck = ConvBlock(features[-1], features[-1]*2)\n",
    "    \n",
    "        # ----- Decoder: Upsampling Path -----\n",
    "        # Reverse features list for symmetrical decoder\n",
    "        rev_feats = features[::-1]\n",
    "        for feat in rev_feats:\n",
    "            # First upsample (via transposed convolution)\n",
    "            self.ups.append(\n",
    "                nn.ConvTranspose1d(feat*2, feat, kernel_size=2, stride=2)\n",
    "            )\n",
    "            # Then apply ConvBlock: input has double channels due to skip connection\n",
    "            self.ups.append(ConvBlock(feat*2, feat))\n",
    "    \n",
    "        # ----- Final Output Convolution -----\n",
    "        # 1x1 convolution to map to desired output channels (e.g., P, S, noise)\n",
    "        self.final_conv = nn.Conv1d(features[0], out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip_connections = []\n",
    "\n",
    "        for down in self.downs:\n",
    "            x = down(x)\n",
    "            skip_connections.append(x)\n",
    "            x = F.max_pool1d(x, kernel_size=2)\n",
    "\n",
    "        x = self.bottleneck(x)\n",
    "        skip_connections = skip_connections[::-1]\n",
    "\n",
    "        for idx in range(0, len(self.ups), 2):\n",
    "            x = self.ups[idx](x)\n",
    "            skip_conn = skip_connections[idx//2]\n",
    "            if x.shape[-1] != skip_conn.shape[-1]:\n",
    "                x = F.pad(x, (0, skip_conn.shape[-1] - x.shape[-1]))\n",
    "            x = torch.cat((skip_conn, x), dim=1)\n",
    "            x = self.ups[idx+1](x)\n",
    "        x = self.final_conv(x)\n",
    "        return F.softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbf5997",
   "metadata": {},
   "source": [
    "We then intialize the model and load in the model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df71ca5-00c2-4fff-960f-f9158f81b684",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet1D()\n",
    "\n",
    "# This cell loads the saved weights into the model.\n",
    "model.load_state_dict(torch.load(\"../Loic/UNet/model_weights_eq_only.pt\",weights_only=True, map_location=torch.device('cpu')))\n",
    "model.eval() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190dc41c",
   "metadata": {},
   "source": [
    "# 2. Load the dataset from the current directory\n",
    "\n",
    "Loads the \"meso\" PNW dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7c7fa3-0bb0-4a81-be5c-9b5c69e684d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sbd.WaveformDataset(\"../data/waveform_dataset/\", component_order=\"ENZ\")\n",
    "data._metadata = data.metadata[data.metadata.source_type == \"earthquake\"].reset_index(drop=True)\n",
    "print(data.metadata[\"source_type\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1481ca",
   "metadata": {},
   "source": [
    "Define the test dataset and the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e138b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = data.test()\n",
    "\n",
    "phase_dict = {\"trace_P_arrival_sample\": \"P\",\n",
    "              \"trace_S_arrival_sample\": \"S\"}\n",
    "\n",
    "model_labels = [\"P\", \"S\", \"noise\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f64d586",
   "metadata": {},
   "source": [
    "# 3. Set up and test the generator\n",
    "\n",
    "Set up generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbaf883-e15e-48b4-96cd-a4ebc2378f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generator = sbg.GenericGenerator(test)\n",
    "\n",
    "augmentations = [\n",
    "    sbg.WindowAroundSample(\n",
    "        list(phase_dict.keys()), samples_before=3000, windowlen=6000,\n",
    "        selection=\"random\", strategy=\"variable\"\n",
    "    ),\n",
    "    sbg.RandomWindow(windowlen=3001, strategy=\"pad\"),\n",
    "    sbg.Normalize(demean_axis=-1, amp_norm_axis=-1, amp_norm_type=\"peak\"),\n",
    "    sbg.ChangeDtype(np.float32),\n",
    "    sbg.ProbabilisticLabeller(label_columns=phase_dict, model_labels=model_labels, sigma=30, dim=0)\n",
    "]\n",
    "\n",
    "test_generator.add_augmentations(augmentations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae67dbdd",
   "metadata": {},
   "source": [
    "Plot generator output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c688e583-47f2-4b8e-a6c6-fdbbfb53cbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_id = 4\n",
    "sample = test_generator[sample_id]\n",
    "# Plot timeseries\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "axs = fig.subplots(2, 1, sharex=True, gridspec_kw={\"hspace\": 0, \"height_ratios\": [3, 1]})\n",
    "axs[0].plot(sample[\"X\"][0].T,label='E',color=\"tab:red\")\n",
    "axs[0].plot(sample[\"X\"][1].T,label='N',color=\"tab:blue\")\n",
    "axs[0].plot(sample[\"X\"][2].T,label='Z',color=\"tab:grey\")\n",
    "axs[0].legend()\n",
    "axs[1].plot(sample[\"y\"][0].T,label='P-wave',color=\"tab:red\")\n",
    "axs[1].plot(sample[\"y\"][1].T,label='S-wave',color=\"tab:blue\")\n",
    "axs[1].plot(sample[\"y\"][2].T,label='Noise',color=\"tab:grey\")\n",
    "axs[1].set_xlabel('Time (s)',fontsize=14)\n",
    "axs[1].set_ylabel('Target Amplitude',fontsize=14)\n",
    "axs[0].set_ylabel('Amplitude',fontsize=14)\n",
    "axs[1].set_xlim((0,3000))\n",
    "axs[1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc8fa84-5534-40c6-805d-625194e889d2",
   "metadata": {},
   "source": [
    "# 4. Make test predictions\n",
    "\n",
    "Next we make predictions on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46190f51-2e0f-4be6-9951-1f8771c28255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seisbench prediction?\n",
    "seisbench_pred=True\n",
    "if seisbench_pred:\n",
    "    pn_model = sbm.PhaseNet.from_pretrained(\"original\")\n",
    "\n",
    "# Define a lambda that returns the index of where the timeseries is maximum\n",
    "def idx_max(ts):\n",
    "    indices = np.where(ts == np.max(ts))[0]\n",
    "    return indices[0], ts[indices[0]]\n",
    "\n",
    "# Generate augmented samples and log their stats\n",
    "targets =[]\n",
    "predictions = []\n",
    "for ii in range(len(test)):\n",
    "    # Initialize variables\n",
    "    p_idx_max, p_val_max, s_idx_max, s_val_max, p_pred_idx_max, s_pred_idx_max, largest_ppeak_idx, largest_speak_idx = -1, -1, -1, -1, -1, -1, -1, -1\n",
    "    # Get the sample\n",
    "    sample = test_generator[ii]\n",
    "    if seisbench_pred:\n",
    "        st=Stream()\n",
    "        for cc, ch in enumerate(['E', 'N', 'Z']):\n",
    "            trace = Trace(data=np.pad(sample['X'][cc], (250, 250), mode='constant', constant_values=0))\n",
    "            trace.stats.sampling_rate = 100\n",
    "            trace.stats.channel = ch\n",
    "            st += trace\n",
    "        preds = pn_model.annotate(st)\n",
    "        noise_pred = preds[0].data\n",
    "        p_pred = preds[1].data\n",
    "        s_pred = preds[2].data\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            pred = model(torch.tensor(sample[\"X\"], device=torch.device('cpu')).unsqueeze(0))  # Add a fake batch dimension\n",
    "            p_pred, s_pred, noise_pred = pred[0].cpu().numpy()\n",
    "\n",
    "    # Find the index of the maximum value in the analyist target\n",
    "    p_idx_max, p_val_max = idx_max(sample[\"y\"][0])\n",
    "    s_idx_max, s_val_max = idx_max(sample[\"y\"][1])\n",
    "\n",
    "    # Find the index of the maximum value in the prediction\n",
    "    p_pred_idx_max, _ = find_peaks(p_pred, height=0.05, distance=100) \n",
    "    s_pred_idx_max, _ = find_peaks(s_pred, height=0.05, distance=100) \n",
    "\n",
    "    # ------------ There is a P-wave prediction and a P-wave target ------------ \n",
    "    if len(p_pred_idx_max) > 0 and p_val_max == 1:\n",
    "        largest_ppeak_idx = p_pred_idx_max[np.argmax(p_pred[p_pred_idx_max])]\n",
    "        predictions.append({\"trace_id\": ii,\n",
    "                    \"pred_max_idx\": largest_ppeak_idx, \n",
    "                    \"pred_val\": p_pred[largest_ppeak_idx],\n",
    "                    \"phase\": \"P\"})         \n",
    "        targets.append({\"trace_id\": ii,\n",
    "                        \"max_idx\": p_idx_max,\n",
    "                        \"phase\": \"P\"})\n",
    "    # There is a P-wave prediction and no P-wave target --> false positive > dt / true negative if < dt\n",
    "    elif len(p_pred_idx_max) > 0 and p_val_max != 1:    \n",
    "        largest_ppeak_idx = p_pred_idx_max[np.argmax(p_pred[p_pred_idx_max])]\n",
    "        predictions.append({\"trace_id\": ii,\n",
    "                    \"pred_max_idx\": largest_ppeak_idx, \n",
    "                    \"pred_val\": p_pred[largest_ppeak_idx],\n",
    "                    \"phase\": \"P\"})         \n",
    "        targets.append({\"trace_id\": ii,\n",
    "                        \"max_idx\": np.nan,\n",
    "                        \"phase\": \"P\"})\n",
    "    # There is no P-wave prediction and a P-wave target --> false negative\n",
    "    elif len(p_pred_idx_max) == 0 and p_val_max == 1:\n",
    "        predictions.append({\"trace_id\": ii,\n",
    "                    \"pred_max_idx\": np.nan, \n",
    "                    \"pred_val\": np.nan,\n",
    "                    \"phase\": \"P\"})         \n",
    "        targets.append({\"trace_id\": ii,\n",
    "                        \"max_idx\": p_idx_max,\n",
    "                        \"phase\": \"P\"})\n",
    "    # There is no P-wave prediction and no P-wave target --> true negative\n",
    "    else:\n",
    "        predictions.append({\"trace_id\": ii,\n",
    "                    \"pred_max_idx\": np.nan, \n",
    "                    \"pred_val\": np.nan,\n",
    "                    \"phase\": \"P\"})         \n",
    "        targets.append({\"trace_id\": ii,\n",
    "                        \"max_idx\": np.nan,\n",
    "                        \"phase\": \"P\"})\n",
    "        \n",
    "    # ------------  There is a S-wave prediction and a S-wave target ------------ \n",
    "    if len(s_pred_idx_max) > 0 and s_val_max == 1:\n",
    "        targets.append({\"trace_id\": ii,\n",
    "                        \"max_idx\": s_idx_max,\n",
    "                        \"phase\": \"S\"})\n",
    "        largest_speak_idx = s_pred_idx_max[np.argmax(s_pred[s_pred_idx_max])]\n",
    "        predictions.append({\"trace_id\": ii,\n",
    "                    \"pred_max_idx\": largest_speak_idx, \n",
    "                    \"pred_val\": s_pred[largest_speak_idx],\n",
    "                    \"phase\": \"S\"})\n",
    "    # There is a S-wave prediction and no S-wave target --> false positive > dt / true negative if < dt\n",
    "    elif len(s_pred_idx_max) > 0 and s_val_max != 1:    \n",
    "        largest_speak_idx = s_pred_idx_max[np.argmax(s_pred[s_pred_idx_max])]\n",
    "        predictions.append({\"trace_id\": ii,\n",
    "                    \"pred_max_idx\": largest_speak_idx, \n",
    "                    \"pred_val\": s_pred[largest_speak_idx],\n",
    "                    \"phase\": \"S\"})         \n",
    "        targets.append({\"trace_id\": ii,\n",
    "                        \"max_idx\": np.nan,\n",
    "                        \"phase\": \"S\"})\n",
    "    # There is no S-wave prediction and a S-wave target --> false negative\n",
    "    elif len(s_pred_idx_max) == 0 and s_val_max == 1:\n",
    "        predictions.append({\"trace_id\": ii,\n",
    "                    \"pred_max_idx\": np.nan, \n",
    "                    \"pred_val\": np.nan,\n",
    "                    \"phase\": \"S\"})         \n",
    "        targets.append({\"trace_id\": ii,\n",
    "                        \"max_idx\": s_idx_max,\n",
    "                        \"phase\": \"S\"})\n",
    "    # There is no S-wave prediction and no S-wave target --> true negative\n",
    "    else:\n",
    "        predictions.append({\"trace_id\": ii,\n",
    "                    \"pred_max_idx\": np.nan, \n",
    "                    \"pred_val\": np.nan,\n",
    "                    \"phase\": \"S\"})         \n",
    "        targets.append({\"trace_id\": ii,\n",
    "                        \"max_idx\": np.nan,\n",
    "                        \"phase\": \"S\"})\n",
    "\n",
    "    # Plot some examples\n",
    "    if ii<10:\n",
    "        # Plot waveforms\n",
    "        fig = plt.figure(figsize=(15, 10))\n",
    "        axs = fig.subplots(2, 1, sharex=True, gridspec_kw={\"hspace\": 0, \"height_ratios\": [3, 1]})\n",
    "        axs[0].plot(sample[\"X\"][0].T,label='E',color=\"tab:red\")\n",
    "        axs[0].plot(sample[\"X\"][1].T,label='N',color=\"tab:blue\")\n",
    "        axs[0].plot(sample[\"X\"][2].T,label='Z',color=\"tab:grey\")\n",
    "        axs[0].text(20,-0.83, f\"P-wave target: {targets[-2][\"max_idx\"]}\", fontsize=10)\n",
    "        axs[0].text(20,-0.88, f\"P-wave prediction: {predictions[-2][\"pred_max_idx\"]}\", fontsize=10)\n",
    "        axs[0].text(20,-0.93, f\"S-wave target: {targets[-1][\"max_idx\"]}\", fontsize=10)\n",
    "        axs[0].text(20,-0.98, f\"S-wave prediction: {predictions[-1][\"pred_max_idx\"]}\", fontsize=10)\n",
    "        axs[0].set_ylim((-1,1))\n",
    "        # Plot target timeseries\n",
    "        axs[1].plot(sample[\"y\"][0].T,label='P-wave target', color=\"tab:red\")\n",
    "        axs[1].plot(sample[\"y\"][1].T,label='S-wave target', color='tab:blue')\n",
    "        if p_val_max == 1:\n",
    "            axs[1].plot(targets[-2][\"max_idx\"], sample[\"y\"][0][targets[-2][\"max_idx\"]], 'o', label='Analyst P-wave', color=\"tab:red\")\n",
    "        if s_val_max == 1:\n",
    "            axs[1].plot(targets[-1][\"max_idx\"], sample[\"y\"][1][targets[-1][\"max_idx\"]], 'o', label='Analyst S-wave', color='tab:blue')\n",
    "        axs[1].plot(p_pred,label='P-wave Prediction', color=\"tab:red\", linestyle='-.')\n",
    "        axs[1].plot(s_pred,label='S-wave Predition', color='tab:blue', linestyle='-.')\n",
    "        if largest_ppeak_idx > 0:\n",
    "            axs[1].plot(predictions[-2][\"pred_max_idx\"],p_pred[predictions[-2][\"pred_max_idx\"]], 'D', label='Predicted P-wave', color=\"tab:red\")\n",
    "        if largest_speak_idx > 0:\n",
    "            axs[1].plot(predictions[-1][\"pred_max_idx\"],s_pred[predictions[-1][\"pred_max_idx\"]], 'D', label='Predicted S-wave', color=\"tab:blue\")\n",
    "        axs[1].set_xlabel('Time (s)',fontsize=14)\n",
    "        axs[1].set_ylabel('Target Amplitude',fontsize=14)\n",
    "        axs[0].set_ylabel('Amplitude',fontsize=14)\n",
    "        # axs[1].plot(noise_pred,label='Noise Prediction', color=\"tab:green\", linestyle='-.')\n",
    "        axs[1].legend()\n",
    "        axs[0].legend()\n",
    "        axs[0].set_xlim((0,3000))\n",
    "        axs[1].set_xlim((0,3000))\n",
    "        axs[0].set_title(f\"Trace ID: {ii}\", fontsize=16)\n",
    "\n",
    "targets = pd.DataFrame(targets)\n",
    "predictions = pd.DataFrame(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffb42e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970ed508",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ec9a1e",
   "metadata": {},
   "source": [
    "# 5.  Common Performance Metrics\n",
    "\n",
    "## The Confusion Matrix\n",
    "\n",
    "The confusion matrix is a tool used to evaluate the performance of a model. It provides insight into the model's performance, errors, and weaknesses which will allow us to further improve our model through fine-tuning.\n",
    "\n",
    "<img src=\"./confusion_matrix.png\" width=\"800\">\n",
    "\n",
    "The four categories in the confusion matrix are:\n",
    "\n",
    "- **True positive (TP)** - The model correctly (true) predicts the positive class (positive).\n",
    "- **False positive (FP)** - The model incorrectly (false) predicts the positive class (positive).\n",
    "- **False negative (FN)** - The model incorrectly (false) predicts the negative class (negative).\n",
    "- **True negative (TN)** - The model correctly (true) predicts the negative class (negative).\n",
    "\n",
    "Using these categories, there are multiple different metrics one can use to assess network perfomance.  For demonstration purposes, lets say we have a dataset of 100 seismograms.  5 of them contain earthquakes, and 95 of them contain noise.  Lets also say of the 5 earthquakes, the model correctly predicts 4 of them and misses 1.  For the 95 noise samples, the model correctly predicts 90 of them as noise and the other 5 it calls earthquakes.  In this scenario, TP = 4, TN = 90, FP = 5, FN = 1.\n",
    "\n",
    "## Accuracy\n",
    "\n",
    "Accuracy shows how often a model is correct overall.  It is defined as the total number of true predictions (i.e. TP + TN) divided by the total number of predictions.\n",
    "\n",
    "$\\huge Accuracy = \\frac{TP + TN}{TP + FP + FN + TN} $\n",
    "\n",
    "In our example above, TP + TN = 94 and the total number of predictions is 100 so we have 94% accuracy overall -- woo hoo!\n",
    "\n",
    "Accuracy is widely used but its not always useful because it treats all classes as equally important. If a dataset is well balanced, meaning it has equal or nearly equal representation in the training data of all classes, accuracy can be a helpful metric. However, many real-world applications have a high imbalance of classes. These are the cases when one category has significantly more frequent occurrences than the other. In seismology, this would happen if you were to just estimate whether each section of a seismogram had an earthquake or recorded only noise. In scenarios like this, you are typically interested in predicting the events that rarely occur (i.e. earthquakes). It is easy to “game” the accuracy metric when making predictions for a dataset like this. To do that, you simply need to predict that every waveform is noise.  If only 5 in 100 waveforms actually contain an earthquake, a model predicting noise all the time will mostly be right, leading to very high accuracy.  In this particular situaion, your model would be 95% accurate however you'd miss every earthquake -- so high accuracy models can be useless. Overall accuracy is informative but using this metric alone is not advisable.\n",
    "\n",
    "## Precision\n",
    "\n",
    "Precision is a metric that measures how often a machine learning model is correct when it predicts the positive class (in our case, earthquakes). You can calculate precision by dividing the number of true positive predictions (TPs) by the total number of instances the model predicted as positive (TP + FP).\n",
    "\n",
    "$\\huge Precision = \\frac{TP}{TP + FP} $\n",
    "\n",
    "In our example above, TP = 4 and FP = 5 so the precition is 4/9 or 44%.  \n",
    "\n",
    "Precision works well for problems with imbalanced classes since it shows the model correctness in identifying the positive class.  It's important to note that precision does not consider false negatives meaning it does not account for the cases when we miss earthquakes.   \n",
    "\n",
    "## Recall\n",
    "\n",
    "Recall is a metric that measures how often a machine learning model correctly identifies positive instances (TP) from all the actual positive samples in the dataset (TP + FN). \n",
    "\n",
    "$\\huge Recall = \\frac{TP}{TP + FN} $\n",
    "\n",
    "From our example above, TP = 4 and FN = 1, so our recall is 4/5 or 80%.\n",
    "\n",
    "Recall works well for problems with imbalanced classes since it is focused on the model’s ability to find earthquakes.  Recall is useful when the cost of false negatives is high. In this case, you typically want to find all earthquakes, even if this results in some false positives (predicting an earthquake when it is actually noise).  A downside is that recall does not account for the cost of these false positives.\n",
    "\n",
    "## F1 score\n",
    "\n",
    "If precision is high but recall is low, it means you're cautious and only pick when confident — but miss many real picks.  If recall is high but precision is low, it means you're aggressive, finding most real picks — but also making lots of false ones.  The F1 score is a metric that combines precision and recall into a single number. It’s especially useful when you want a balance between these two, particularly in imbalanced datasets — like seismic phase picking, where picks are rare compared to the number of time steps.\n",
    "\n",
    "$\\huge F1 = \\frac{2*precision*recall}{precision + recall}$\n",
    "\n",
    "F1 gives a harmonic mean, so it heavily penalizes large gaps between precision and recall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6c67b0",
   "metadata": {},
   "source": [
    "# 6. Compute UNet Performance metrics\n",
    "\n",
    "Looking back at the original Zhu and Beroza [2019] paper that introduced PhaseNet, they have a little bit of a problem.  They've set up the earthquake detection problem as a regression where the network outputs a \"probability\" between 0 and 1 to identify noise (0) vs. a phase arrival (1).  So how do we translate this into a classicifcation problem so that we can use the swanky metrics we introduced above?  Well they did it as follows (and so will we!):\n",
    "\n",
    "- **True positive** - peak amplitude is above decision threshold and < 0.1s from true arrival\n",
    "- **False positive** - peak amplitude is above decision threshold and >= 0.1s from true arrival\n",
    "- **False negative** - peak amplitude is below decision threshold and < 0.1s from true arrival\n",
    "- **True negative** - peak amplitude is below decision threshold and >= 0.1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8ec4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.merge(\n",
    "    predictions, \n",
    "    targets, \n",
    "    how=\"inner\", \n",
    "    on=[\"trace_id\", \"phase\"],\n",
    "    suffixes=('_pred', '_true')\n",
    ")\n",
    "\n",
    "# Delete for speed\n",
    "del predictions\n",
    "del targets\n",
    "\n",
    "# Dealing with the NaNs\n",
    "mask=False\n",
    "if mask:\n",
    "    no_pred_and_no_target = (\n",
    "        merged[\"pred_max_idx\"].isna() & \n",
    "        merged[\"max_idx\"].isna() & \n",
    "        merged[\"pred_val\"].isna()\n",
    "    )\n",
    "\n",
    "    target_no_pred = (\n",
    "        merged[\"pred_max_idx\"].isna() & \n",
    "        merged[\"max_idx\"].notna() & \n",
    "        merged[\"pred_val\"].isna()\n",
    "    )\n",
    "\n",
    "    pred_no_target = (\n",
    "        merged[\"pred_max_idx\"].notna() & \n",
    "        merged[\"max_idx\"].isna() & \n",
    "        merged[\"pred_val\"].notna()\n",
    "    )\n",
    "\n",
    "    # Apply masks\n",
    "    merged.loc[no_pred_and_no_target, [\"pred_max_idx\", \"max_idx\", \"pred_val\"]] = [0, 100, 0]\n",
    "    merged.loc[target_no_pred, [\"pred_max_idx\", \"max_idx\", \"pred_val\"]] = [0, 0, 0]\n",
    "    merged.loc[pred_no_target, [\"pred_max_idx\", \"max_idx\"]] = [0, 100]\n",
    "else:\n",
    "    merged=merged.dropna()\n",
    "\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    print(merged)\n",
    "\n",
    "# Check for NaN values in the merged DataFrame\n",
    "merged[merged.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2231187b",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_thresholds = np.linspace(0.05, 0.95, num=19)\n",
    "pprecision = np.zeros(len(decision_thresholds))\n",
    "precall = np.zeros(len(decision_thresholds))\n",
    "sprecision = np.zeros(len(decision_thresholds))\n",
    "srecall = np.zeros(len(decision_thresholds))\n",
    "\n",
    "for phase in [\"P\", \"S\"]:\n",
    "    phase_data = merged[merged[\"phase\"] == phase]\n",
    "    for ii, decision_threshold in enumerate(decision_thresholds):\n",
    "        above_thresh = phase_data[\"pred_val\"] >= decision_threshold\n",
    "        close_enough = (phase_data[\"pred_max_idx\"] - phase_data[\"max_idx\"]).abs() < 10\n",
    "\n",
    "        TP = ((above_thresh) & (close_enough)).sum()\n",
    "        FP = ((above_thresh) & (~close_enough)).sum()\n",
    "        FN = ((~above_thresh) & (close_enough)).sum()\n",
    "        TN = ((~above_thresh) & (~close_enough)).sum()\n",
    "\n",
    "        # Save or print results here as before  \n",
    "        print(f\"Decision Threshold: {decision_threshold}, TP: {TP}, FP: {FP}, FN: {FN}, TN: {TN}, Total: {TN+FN+TP+FP}\")\n",
    "\n",
    "        if phase == \"P\":\n",
    "            pprecision[ii] = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "            precall[ii] = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "        elif phase == \"S\":\n",
    "            sprecision[ii] = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "            srecall[ii] = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "\n",
    "pF1=2*pprecision*precall/(pprecision+precall)\n",
    "sF1=2*sprecision*srecall/(sprecision+srecall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a50d94d",
   "metadata": {},
   "source": [
    "# 7. Plot precision, recall, and F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7be6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig, axs = plt.subplots(\n",
    "    1, 2,              # 1 row, 2 columns\n",
    "    figsize=(15, 6),  # overall figure size\n",
    "    sharey=True,       # share y-axis\n",
    "    gridspec_kw={\"width_ratios\": [1, 1], \"wspace\": 0.1}  # control layout\n",
    ")\n",
    "decision_thresholds = np.linspace(0.05, 0.95, num=19)\n",
    "axs[0].plot(decision_thresholds, pprecision, label='P Precision', color=\"tab:red\")\n",
    "axs[0].plot(decision_thresholds, precall, label='P Recall', color=\"tab:blue\")\n",
    "axs[0].plot(decision_thresholds, pF1, label='P F1', color=\"tab:green\")\n",
    "axs[0].set_xlim(0,1)\n",
    "axs[0].set_ylim(0,1)\n",
    "axs[0].legend()\n",
    "axs[0].set_title(\"P-wave\", fontsize=14)\n",
    "axs[1].set_title(\"S-wave\", fontsize=14)\n",
    "axs[1].plot(decision_thresholds, sprecision, label='S Precision', color=\"tab:red\")\n",
    "axs[1].plot(decision_thresholds, srecall, label='S Recall', color=\"tab:blue\")\n",
    "axs[1].plot(decision_thresholds, sF1, label='S F1', color=\"tab:green\")\n",
    "axs[1].set_xlim(0,1)\n",
    "axs[1].set_ylim(0,1)\n",
    "axs[0].set_xlabel('Decision Threshold', fontsize=14)\n",
    "axs[1].set_xlabel('Decision Threshold', fontsize=14)\n",
    "axs[1].legend()\n",
    "\n",
    "ind=9\n",
    "print(\"P Precision = \"+str(pprecision[ind]))\n",
    "print(\"P Recall = \"+str(precall[ind]))\n",
    "print(\"S Precision = \"+str(sprecision[ind]))\n",
    "print(\"S Recall = \"+str(srecall[ind]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a4eb7f",
   "metadata": {},
   "source": [
    "from IPython.display import Image, display\n",
    "display(Image(filename='./table1.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861f1397",
   "metadata": {},
   "source": [
    "For reference here are the specs from the original PhaseNet paper.\n",
    "\n",
    "![Table 1](./table1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095c8209",
   "metadata": {},
   "source": [
    "# 7. Compute and plot the pick distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b863776e",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_thresholds=[0.5,0.7,0.9]\n",
    "\n",
    "# Plot the histograms\n",
    "colors = [[0.5,0,0],[0,0,0.5],[0,0.5,0]]\n",
    "fig = plt.figure()\n",
    "fig, axs = plt.subplots(\n",
    "    1, 2,              # 1 row, 2 columns\n",
    "    figsize=(15, 6),  # overall figure size\n",
    "    sharey=True,       # share y-axis\n",
    "    gridspec_kw={\"width_ratios\": [1, 1], \"wspace\": 0.1}  # control layout\n",
    ")\n",
    "\n",
    "for count, decision_threshold in enumerate(decision_thresholds):\n",
    "    # Decision threshold\n",
    "    phase=\"P\"\n",
    "    phase_data = merged[(merged[\"phase\"] == phase) & (merged[\"pred_val\"] >= decision_threshold) & (merged[\"max_idx\"] != -100)]\n",
    "    p_pick_diff = phase_data[\"pred_max_idx\"] - phase_data[\"max_idx\"]\n",
    "    phase=\"S\"\n",
    "    phase_data = merged[(merged[\"phase\"] == phase) & (merged[\"pred_val\"] >= decision_threshold) & (merged[\"max_idx\"] != -100)]\n",
    "    s_pick_diff = phase_data[\"pred_max_idx\"] - phase_data[\"max_idx\"]\n",
    "\n",
    "    axs[0].hist(p_pick_diff, bins=np.arange(-30.5,30.5,1), rwidth=0.8, color=colors[count], alpha=0.5, edgecolor='black', label=\"decision threshold =\"+'% 6.2f' % decision_threshold)\n",
    "    axs[1].hist(s_pick_diff, bins=np.arange(-30.5,30.5,1), rwidth=0.8, color=colors[count], alpha=0.5, edgecolor='black', label=\"decision threshold =\"+'% 6.2f' % decision_threshold)\n",
    "plt.legend()\n",
    "axs[0].set_xlabel(\"Residual [samples]\",fontsize=14)\n",
    "axs[0].set_title(\"P wave\",fontsize=14)\n",
    "axs[1].set_title(\"S wave\",fontsize=14)\n",
    "axs[0].set_ylabel(\"Frequency\",fontsize=14)\n",
    "axs[1].set_xlabel(\"Residual [samples]\",fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5598f986",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
