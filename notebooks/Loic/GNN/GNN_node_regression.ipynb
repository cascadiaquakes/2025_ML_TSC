{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# üåê GNN for Node-Level Regression: Predicting Sine Waves\n",
    "\n",
    "**Author:** Lo√Øc Bachelot  \n",
    "**Goal:** This notebook demonstrates how to use a GNN to predict 1D sine wave signals at each node of a synthetic graph.\n",
    "\n",
    "---\n",
    "\n",
    "## üìò Overview\n",
    "\n",
    "Graphs provide a natural framework to represent structured spatial relationships, like seismic stations or sensor networks.\n",
    "\n",
    "In this notebook:\n",
    "- Each node in the graph has spatial coordinates and a local signal (a sine wave).\n",
    "- The task is to predict the full sine wave **at each node** based on the graph structure and node features.\n",
    "- We use **GNNs** with message passing to exploit the spatial relationships between nodes.\n",
    "\n",
    "We will:\n",
    "- Generate synthetic 3D graphs with signals delayed based on spatial distance\n",
    "- Define a GNN model for **node-level regression**\n",
    "- Train the model to recover sine signals\n",
    "- Visualize and evaluate the predictions at the node level\n",
    "\n",
    "This setup introduces important concepts for modeling spatiotemporal data with graph-based deep learning!\n"
   ],
   "id": "a9d8a35ced2ce811"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## üì¶ Imports and Setup\n",
    "\n",
    "We first import the core libraries needed for:\n",
    "\n",
    "- Graph data manipulation (`torch_geometric`)\n",
    "- Deep learning (`torch`)\n",
    "- Numerical operations (`numpy`)\n",
    "- Visualization (`matplotlib`)\n",
    "\n",
    "This setup ensures we have all the tools needed for dataset generation, model training, and evaluation."
   ],
   "id": "a80d915564e5f8a4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pickle\n",
    "import shutil\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.data import Data\n",
    "import torch\n",
    "import torch_geometric\n",
    "from torch_geometric.transforms import KNNGraph\n",
    "from torch_geometric.data import InMemoryDataset, Data\n",
    "from torch.nn import Linear, Parameter, LeakyReLU, Conv2d, MaxPool1d, Conv1d\n",
    "from torch_geometric.nn import GCNConv, MessagePassing, MLP, GATv2Conv  \n",
    "from scipy.spatial import distance\n",
    "from torch_geometric.utils import add_self_loops, degree\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import random"
   ],
   "id": "189b2474425f0f4a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### üñºÔ∏è Visualization Helper Functions\n",
    "\n",
    "We define two visualization utilities to better explore and understand the synthetic graphs.\n",
    "\n",
    "---\n",
    "\n",
    "### üåê 1. `visualize_graph_torch()`\n",
    "\n",
    "Plots the graph structure with flexible coloring options.\n",
    "\n",
    "- **Edges** are plotted as light gray lines connecting nodes.\n",
    "- **Nodes** are colored based on:\n",
    "  - An internal attribute (e.g., `\"signal\"`, `\"attention\"`)\n",
    "  - Or external per-node values (e.g., prediction errors).\n",
    "- **Origin** (hidden point that controls the delay) is plotted as a **red cross (√ó)** if available.\n",
    "- A colorbar is automatically generated to aid interpretation.\n",
    "\n",
    "‚úÖ Useful for visualizing spatial structure, feature distributions, and model errors.\n",
    "\n",
    "---\n",
    "\n",
    "### üìà 2. `plot_signals_subplots_by_distance()`\n",
    "\n",
    "Visualizes the individual node signals sorted by distance from the hidden origin.\n",
    "\n",
    "- **One subplot per node**: each shows the sine waveform observed at that node.\n",
    "- Nodes are **sorted** by increasing distance to the origin.\n",
    "- A **vertical red dashed line** marks the expected arrival time of the sine wave based on distance and velocity.\n",
    "\n",
    "‚úÖ Useful for understanding how distance delays the waveform across the graph.\n",
    "\n",
    "---\n"
   ],
   "id": "62065b56eaf68873"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def visualize_graph_torch(g, color=None, node_values=None, cmap=\"viridis\", ax=None, title=None):\n",
    "    \"\"\"\n",
    "    Visualize a graph structure with nodes colored by a selected attribute or external values.\n",
    "\n",
    "    Args:\n",
    "        g (Data): PyG graph object with at least 'edge_index' and 'pos'. May also have 'origin'.\n",
    "        color (str, optional): Node attribute key inside `g` to color nodes (e.g., 'signal').\n",
    "        node_values (Tensor, optional): External array of node values for coloring (overrides `color` if provided).\n",
    "        cmap (str, optional): Matplotlib colormap name for node coloring.\n",
    "        ax (matplotlib axis, optional): Axis to plot into (if None, create a new figure).\n",
    "        title (str, optional): Title for the plot.\n",
    "\n",
    "    Behavior:\n",
    "        - Nodes are colored based on either an internal attribute or external values.\n",
    "        - Edges are drawn as light gray lines between connected nodes.\n",
    "        - A colorbar is added automatically when node coloring is used.\n",
    "        - If `origin` is present, it is shown as a red \"x\" marker.\n",
    "    \"\"\"\n",
    "\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "    # --- Draw edges ---\n",
    "    for edge in g.edge_index.T:\n",
    "        ax.plot(\n",
    "            [g.pos[edge[0]][0], g.pos[edge[1]][0]],\n",
    "            [g.pos[edge[0]][1], g.pos[edge[1]][1]],\n",
    "            color='lightgray', linewidth=0.8, zorder=0\n",
    "        )\n",
    "\n",
    "    # --- Determine node coloring ---\n",
    "    if node_values is not None:\n",
    "        node_colors = node_values\n",
    "    elif color is not None:\n",
    "        node_colors = g[color][:, 0].cpu().numpy()\n",
    "    else:\n",
    "        node_colors = \"blue\"  # fallback color\n",
    "\n",
    "    # --- Draw nodes ---\n",
    "    scatter = ax.scatter(\n",
    "        g.pos[:, 0].cpu(),\n",
    "        g.pos[:, 1].cpu(),\n",
    "        c=node_colors,\n",
    "        cmap=cmap,\n",
    "        s=150,\n",
    "        edgecolors='black'\n",
    "    )\n",
    "\n",
    "    # --- Plot origin if available ---\n",
    "    if hasattr(g, 'origin'):\n",
    "        origin = g.origin.cpu().numpy()\n",
    "        ax.plot(origin[0], origin[1], 'rx', markersize=12, markeredgewidth=3, label=\"Origin\")\n",
    "        ax.legend(loc=\"upper right\")\n",
    "\n",
    "    # --- Add colorbar if appropriate ---\n",
    "    if node_values is not None or color is not None:\n",
    "        plt.colorbar(scatter, ax=ax, label=\"Node Value\")\n",
    "\n",
    "    if title:\n",
    "        ax.set_title(title)\n",
    "\n",
    "    ax.set_xlabel(\"x\")\n",
    "    ax.set_ylabel(\"y\")\n",
    "    ax.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "    if ax is None:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def plot_signals_subplots_by_distance(data, velocity=0.25, sampling_rate=1.0, title=\"Signals by distance to origin\"):\n",
    "    \"\"\"\n",
    "    Plot each node's signal in its own subplot, ordered by distance to the hidden origin.\n",
    "\n",
    "    Args:\n",
    "        data (Data): PyG graph object containing:\n",
    "            - pos: [num_nodes, 2] node spatial coordinates\n",
    "            - signal: [num_nodes, signal_size] node waveforms\n",
    "            - origin: [2] hidden origin point (required to compute distance)\n",
    "        velocity (float): Wave propagation speed (units per second).\n",
    "        sampling_rate (float): Sampling rate in Hz (samples per second).\n",
    "        title (str): Figure title.\n",
    "\n",
    "    Behavior:\n",
    "        - Sorts nodes by their distance to the origin.\n",
    "        - Plots each node's signal in a separate subplot.\n",
    "        - Marks the expected arrival sample with a red vertical line.\n",
    "    \"\"\"\n",
    "    pos = data.pos.cpu().numpy()\n",
    "    signals = data.signal.cpu().numpy()\n",
    "    origin = data.origin.squeeze().cpu().numpy()\n",
    "\n",
    "    num_nodes = pos.shape[0]\n",
    "\n",
    "    # Compute distances from origin and convert to sample index\n",
    "    distances = np.array([distance.euclidean(origin, pos[i].tolist()) for i in range(num_nodes)])\n",
    "    arrival_samples = (distances / velocity * sampling_rate).astype(int)\n",
    "    sort_idx = np.argsort(distances)\n",
    "\n",
    "    # Plot one subplot per station\n",
    "    fig, axs = plt.subplots(num_nodes, 1, figsize=(10, 2 * num_nodes), sharex=True)\n",
    "    for i, idx in enumerate(sort_idx):\n",
    "        ax = axs[i]\n",
    "        signal = signals[idx]\n",
    "\n",
    "        ax.plot(np.arange(len(signal)), signal, color='black', linewidth=1)\n",
    "        ax.axvline(arrival_samples[idx], color='red', linestyle='--', linewidth=1, label='arrival')\n",
    "        ax.set_ylabel(f\"{distances[idx]:.2f}\", rotation=0, labelpad=25)\n",
    "        ax.grid(True, linestyle='--', alpha=0.3)\n",
    "\n",
    "    axs[-1].set_xlabel(\"Time (samples)\")\n",
    "    fig.suptitle(title, fontsize=14)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.98])\n",
    "    plt.show()"
   ],
   "id": "18e9ed7db6b14970"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## üßπ Synthetic Dataset Creation\n",
    "\n",
    "We create a synthetic graph dataset where each node has:\n",
    "- Random 2D spatial coordinates\n",
    "- A **sine wave** signal, delayed according to its distance from a hidden origin point\n",
    "\n",
    "The task is to predict the correct sine signal at each node, using the spatial structure and nearby signals as context.\n",
    "\n",
    "---\n",
    "\n",
    "### üõ†Ô∏è Components:\n",
    "\n",
    "1. **`add_edge_weight(g)`**  \n",
    "   A utility function that computes edge weights based on inverse spatial distance, helping GNN layers better prioritize closer nodes.\n",
    "\n",
    "2. **`SinDataset` Class**  \n",
    "   A PyTorch Geometric `InMemoryDataset` that:\n",
    "   - Generates random graphs\n",
    "   - Assigns sine signals to nodes with distance-based delays\n",
    "   - Saves origin coordinate of signal\n",
    "   - Optionally applies transformations like edge construction\n",
    "\n",
    "This synthetic setup allows us to benchmark node-level regression performance.\n"
   ],
   "id": "7f2a86d6c76cd828"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def add_edge_weight(g):\n",
    "    \"\"\"\n",
    "    Compute edge features as (dx, dy) vectors between connected nodes.\n",
    "    \"\"\"\n",
    "    edge_attrs = []\n",
    "\n",
    "    for edge in g.edge_index.T:\n",
    "        src = g.pos[edge[0]]\n",
    "        tgt = g.pos[edge[1]]\n",
    "        delta = tgt - src  # (dx, dy)\n",
    "        edge_attrs.append(delta)\n",
    "\n",
    "    # Stack into [num_edges, 2] tensor\n",
    "    g.edge_attr = torch.stack(edge_attrs, dim=0).type(torch.float32)\n",
    "    return g\n",
    "\n",
    "\n",
    "class SinDataset(InMemoryDataset):\n",
    "    \"\"\"\n",
    "    Synthetic dataset for node-level sine wave reconstruction,\n",
    "    using a **fixed set of node positions (\"stations\")** across all graphs.\n",
    "\n",
    "    Each graph contains:\n",
    "    - Same station layout (pos)\n",
    "    - Different random hidden origin\n",
    "    - Signals delayed based on distance to origin\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root, transform=None, pre_transform=None, nb_graph=10, nb_stations=15):\n",
    "        self.nb_graph = nb_graph\n",
    "        self.nb_stations = nb_stations\n",
    "\n",
    "        # Generate fixed station positions once\n",
    "        self.node_positions = torch.tensor(\n",
    "            np.random.uniform(0, 6, size=(self.nb_stations, 2)),\n",
    "            dtype=torch.float\n",
    "        )\n",
    "        super(SinDataset, self).__init__(root, transform, pre_transform)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0], weights_only=False)\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        # No external raw files\n",
    "        return 0\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return 'data.pt'\n",
    "\n",
    "    def process(self):\n",
    "        data_list = []\n",
    "\n",
    "        # Dataset parameters\n",
    "        frequency = 1.0        # Hz\n",
    "        velocity = 0.25         # wave speed\n",
    "        sampling_rate = 1.0    # Hz\n",
    "        noise_std = 0.05        # Gaussian noise level\n",
    "\n",
    "        # Base sine wave used in signal synthesis\n",
    "        base_sine_wave = np.sin(np.arange(SIGNAL_SIZE))\n",
    "\n",
    "        for _ in range(self.nb_graph):\n",
    "            pos = self.node_positions\n",
    "\n",
    "            # Random hidden origin point\n",
    "            origin = np.random.randint(0, 6, size=2)\n",
    "            origin_tensor = torch.tensor(origin, dtype=torch.float)\n",
    "\n",
    "            signal_list = []\n",
    "            for i in range(self.nb_stations):\n",
    "                dist = distance.euclidean(origin, pos[i].tolist())\n",
    "                delay = dist / velocity\n",
    "                delay_samples = int(delay * sampling_rate)\n",
    "\n",
    "                # Start with noise\n",
    "                waveform = np.random.normal(0, noise_std, size=SIGNAL_SIZE)\n",
    "\n",
    "                # Add delayed sine wave if within bounds\n",
    "                if delay_samples < SIGNAL_SIZE:\n",
    "                    insert_length = SIGNAL_SIZE - delay_samples\n",
    "                    waveform[delay_samples:] += base_sine_wave[:insert_length]\n",
    "\n",
    "                signal_list.append(waveform)\n",
    "\n",
    "            signal = torch.tensor(np.array(signal_list), dtype=torch.float32).reshape(self.nb_stations, SIGNAL_SIZE)\n",
    "            g = Data(pos=pos, signal=signal, origin=origin_tensor)  # y shape: (1, 2)\n",
    "            data_list.append(g)\n",
    "            \n",
    "        # Apply preprocessing transformations if specified\n",
    "        if self.pre_transform is not None:\n",
    "            data_list = [self.pre_transform(data) for data in data_list]\n",
    "            data_list = [add_edge_weight(data) for data in data_list]\n",
    "\n",
    "        # Save processed graphs\n",
    "        data, slices = self.collate(data_list)\n",
    "        torch.save((data, slices), self.processed_paths[0])"
   ],
   "id": "ef6adf9e7aae7e5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Parameters\n",
    "SIGNAL_SIZE = 25\n",
    "NB_GRAPHS = 5000\n",
    "NB_STATION = 30\n",
    "\n",
    "# Dataset root path\n",
    "dataset_root = \"./sin_train_masked\"\n",
    "\n",
    "# üö® Remove old processed data if it exists\n",
    "processed_path = os.path.join(dataset_root, \"processed\")\n",
    "if os.path.exists(processed_path):\n",
    "    print(f\"Removing previously processed dataset at {processed_path}...\")\n",
    "    shutil.rmtree(processed_path)\n",
    "\n",
    "# Create fresh dataset\n",
    "dataset = SinDataset(\n",
    "    root=dataset_root,\n",
    "    pre_transform=KNNGraph(k=6, loop=False, force_undirected=True),\n",
    "    nb_graph=NB_GRAPHS, \n",
    "    nb_stations=NB_STATION\n",
    ")\n",
    "\n",
    "dataset"
   ],
   "id": "c5bfe527d8fddbd2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### üîç Exploring a Sample Graph\n",
    "\n",
    "Let's inspect one sample from the synthetic dataset to understand its structure.\n",
    "\n",
    "Each graph is stored as a PyTorch Geometric `Data` object, which includes:\n",
    "- **`pos`**: Node 2D spatial coordinates `[num_nodes, 2]`\n",
    "- **`signal`**: Node signals (sine waves delayed by distance to a hidden origin) `[num_nodes, signal_length]`\n",
    "- **`origin`**: The hidden 2D origin point `[2]` used to generate node signals (not directly used for supervision here)\n",
    "- **`edge_index`**: Connectivity information between nodes (edges)\n",
    "- **`edge_attr`**: distance in x and y coordinate between connected nodes\n",
    "\n",
    "This compact graph-based representation allows flexible training for node-level prediction tasks,  \n",
    "while preserving spatial and relational structure essential for learning.\n"
   ],
   "id": "cdf2a166a2766244"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "data = dataset[0]\n",
    "data"
   ],
   "id": "68e03a59088208c1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "visualize_graph_torch(data, color='signal')",
   "id": "5b2492dd01b3ee1d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "plot_signals_subplots_by_distance(data)",
   "id": "7e3c89796fbaf089"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### üß™ Train/Validation/Test Split and Batching\n",
    "\n",
    "After generating the full synthetic dataset, we split it into three subsets:\n",
    "\n",
    "- **80%** for training\n",
    "- **10%** for validation\n",
    "- **10%** for testing\n",
    "\n",
    "We then use `torch_geometric.loader.DataLoader` to batch graphs during training and evaluation.\n",
    "\n",
    "#### üß± How batching works in PyTorch Geometric:\n",
    "- Nodes, edges, and features from multiple graphs are merged into a single \"big graph.\"\n",
    "- A `batch` vector tracks which nodes belong to which original graph.\n",
    "- This enables efficient parallel processing across many small graphs in one forward pass.\n",
    "\n",
    "We set a `batch_size=64` and shuffle only the training set to ensure randomness while preserving validation/test consistency.\n"
   ],
   "id": "cd3347f72cbb2153"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_dataset, val_dataset = train_test_split(dataset, train_size=0.8, random_state=42)\n",
    "val_dataset, test_dataset = train_test_split(val_dataset, train_size=0.5, random_state=42)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "print(f\"nb graph train ds= {len(train_dataset)}, nb graph val ds= {len(val_dataset)}, nb graph test ds= {len(test_dataset)}\")"
   ],
   "id": "fe4b12ad750577c3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## üß† GNN Architecture: BasicNet (2 GATv2 Layers + Dual Attention)\n",
    "\n",
    "This model performs **node-level signal reconstruction** based purely on graph structure and partially masked inputs.\n",
    "\n",
    "---\n",
    "\n",
    "### üìö Model Overview\n",
    "\n",
    "- **Input**:  \n",
    "  - Node spatial coordinates `(x, y)`\n",
    "  - Node signal (possibly masked)\n",
    "\n",
    "- **Feature Fusion**:\n",
    "  - Concatenate spatial features and masked signals\n",
    "  - Pass through a small **MLP** to compress into a hidden representation\n",
    "\n",
    "- **Graph Message Passing**:\n",
    "  - **First GATv2Conv** layer with 2 attention heads\n",
    "    - Aggregates neighbor information with rich directional context (using `(dx, dy)` edge features)\n",
    "  - **Second GATv2Conv** layer with 2 heads\n",
    "    - Further refines each node's hidden representation by second-order neighborhood aggregation\n",
    "\n",
    "- **Decoding**:\n",
    "  - Another **MLP** expands the hidden representation back to the original signal shape\n",
    "\n",
    "- **Output**:\n",
    "  - Final output is passed through `tanh()` to ensure reconstructed signals are within `[-1, 1]` (matching the sine wave amplitudes)\n",
    "\n",
    "---\n",
    "\n",
    "### üî• Key Features\n",
    "\n",
    "| Component | Details |\n",
    "|-----------|---------|\n",
    "| Input masking | Prevents nodes from \"cheating\" by using their own input signal |\n",
    "| Dual attention heads | Each node aggregates information through multiple relational paths |\n",
    "| `(dx, dy)` edge attributes | Direction-aware message passing |\n",
    "| Two GATv2Conv layers | Allows multi-hop neighbor information propagation |\n",
    "| Lightweight MLPs | Efficient feature transformation and decoding |\n",
    "| tanh output activation | Matches the physical constraint of sine wave amplitudes |\n",
    "\n",
    "---\n",
    "\n",
    "### üõ†Ô∏è Dynamic Masking Mechanism\n",
    "\n",
    "During training:\n",
    "- Randomly select a subset of nodes to supervise.\n",
    "- Their own input signal is zeroed out (masked).\n",
    "- The model reconstructs the missing signals using neighbor information.\n",
    "\n",
    "This trains the GNN to **inpaint missing signals** based on graph connectivity ‚Äî  \n",
    "similar to masked autoencoders in vision, but adapted for graphs!\n",
    "\n",
    "---\n"
   ],
   "id": "e5ab5ccc8bfa3e6a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class BasicNet(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A minimal Graph Neural Network for node-level signal reconstruction.\n",
    "\n",
    "    Architecture:\n",
    "    - MLP to fuse spatial coordinates and raw signal\n",
    "    - Single GATv2Conv layer for graph message passing\n",
    "    - MLP decoder to reconstruct the node signals\n",
    "    \"\"\"\n",
    "    def __init__(self, channels_x, channels_y, hidden_channels=32, dropout=0.3):\n",
    "        super(BasicNet, self).__init__()\n",
    "        torch.manual_seed(1234)\n",
    "\n",
    "        # --- Feature fusion ---\n",
    "        self.mlp_in = MLP([channels_x + channels_y, 128, hidden_channels])\n",
    "\n",
    "        # --- Graph convolution ---\n",
    "        self.conv = GATv2Conv(\n",
    "            in_channels=hidden_channels,\n",
    "            out_channels=hidden_channels,\n",
    "            heads=2,\n",
    "            concat=True,\n",
    "            edge_dim=2\n",
    "        )\n",
    "        self.conv2 = GATv2Conv(\n",
    "            hidden_channels*2, \n",
    "            hidden_channels, \n",
    "            heads=2, \n",
    "            concat=True, \n",
    "            edge_dim=2\n",
    "        )\n",
    "\n",
    "        # --- Decoder ---\n",
    "        self.mlp_out = MLP([hidden_channels*2, 128, channels_y])\n",
    "\n",
    "    def forward(self, x, signal, edge_index, edge_attr, mask):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Node features (e.g., spatial coordinates) [num_nodes, channels_x]\n",
    "            signal (Tensor): Node signals (raw) [num_nodes, channels_y]\n",
    "            edge_index (Tensor): Edge index for the graph\n",
    "            edge_weight (Tensor): Edge weights\n",
    "            mask (Tensor): Mask indicating supervised nodes\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Reconstructed node signals [num_nodes, channels_y]\n",
    "        \"\"\"\n",
    "        # Apply mask (simulate missing inputs)\n",
    "        signal = (signal.T * (~mask)).T\n",
    "\n",
    "        # --- Feature fusion ---\n",
    "        out = torch.cat([x, signal], dim=-1)\n",
    "        out = self.mlp_in(out)\n",
    "\n",
    "        # --- Graph convolution ---\n",
    "        out = self.conv(out, edge_index, edge_attr)\n",
    "        out = self.conv2(out, edge_index, edge_attr)\n",
    "\n",
    "        # --- Final decoding ---\n",
    "        out = self.mlp_out(out)\n",
    "\n",
    "        return out.tanh()\n",
    "\n",
    "    @staticmethod\n",
    "    def get_mask(graph_size: int, rate: float, device):\n",
    "        \"\"\"\n",
    "        Randomly generate a node mask for supervision during training.\n",
    "    \n",
    "        Args:\n",
    "            graph_size (int): Number of nodes in the graph.\n",
    "            rate (float): Fraction of nodes to supervise (between 0 and 1).\n",
    "            device: Target device (CPU or GPU).\n",
    "    \n",
    "        Returns:\n",
    "            Tensor: A boolean mask indicating which nodes to supervise.\n",
    "        \"\"\"\n",
    "        mask = torch.rand(graph_size, device=device) < rate\n",
    "        return mask"
   ],
   "id": "d6264babb941aa60"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## üöÄ Training Loop\n",
    "\n",
    "This section defines the training and validation routines for our GNN model, as well as a simple early stopping mechanism."
   ],
   "id": "954f2b462bda4746"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### üõ†Ô∏è Training and Validation Helper Functions\n",
    "\n",
    "Before launching training, we define helper functions to structure the training and validation process.\n",
    "\n",
    "### ‚èπÔ∏è Early Stopping: `EarlyStopper`\n",
    "Early stopping monitors the **validation loss** to detect overfitting.  \n",
    "It stops training when the model stops improving for a given number of epochs (`patience`).\n",
    "\n",
    "This helps avoid:\n",
    "- Wasting compute resources\n",
    "- Overfitting to the training set\n",
    "\n",
    "---\n",
    "\n",
    "### üõ†Ô∏è Training Function: `train()`\n",
    "\n",
    "For each batch of graphs:\n",
    "1. **Dynamically generate a random mask** selecting a subset of nodes for supervision.\n",
    "2. Perform a forward pass through the model **using the generated mask** (the mask modifies the input signals).\n",
    "3. Compute the loss **only on the masked (supervised) nodes**.\n",
    "4. Backpropagate and update model parameters.\n",
    "\n",
    "> üß† **Why use a dynamic mask?**  \n",
    "> - In node-level prediction, we don't want to supervise all nodes.\n",
    "> - In this use case, the target is the input itself, so without masking, a node could trivially copy its input!\n",
    "> - Dynamic masking ensures the model must infer missing signals by leveraging neighborhood information.\n",
    "> - Different nodes are randomly selected in each batch, improving generalization.\n",
    "\n",
    "The loss is computed **only** for the nodes selected by the dynamic mask.\n",
    "\n",
    "---\n",
    "\n",
    "### üß™ Validation Function: `validation()`\n",
    "\n",
    "During validation:\n",
    "- We also **dynamically generate a new random mask** per batch.\n",
    "- This ensures evaluation matches training conditions (masked input, selective supervision).\n",
    "- Gradients are disabled using `@torch.no_grad()` for efficiency.\n",
    "\n",
    "> ‚ö° Note:  \n",
    "> Message passing still uses the full graph (all nodes), but supervision happens **only on the masked nodes**!\n",
    "\n",
    "---\n"
   ],
   "id": "be293c732007c7f7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class EarlyStopper:\n",
    "    \"\"\"\n",
    "    A class for early stopping the training process when the validation loss stops improving.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    patience : int, optional (default=1)\n",
    "        The number of epochs with no improvement in validation loss after which training will be stopped.\n",
    "    min_delta : float, optional (default=0)\n",
    "        The minimum change in the validation loss required to qualify as an improvement.\n",
    "    \"\"\"\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = np.inf\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        \"\"\"\n",
    "        Check if the training process should be stopped.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        validation_loss : float\n",
    "            The current validation loss.\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        stop : bool\n",
    "            Whether the training process should be stopped or not.\n",
    "        \"\"\"\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "\n",
    "def train(dataloader, device, mask_rate=0.1):\n",
    "    \"\"\"\n",
    "    Train the model with dynamic masking per batch.\n",
    "\n",
    "    Args:\n",
    "        dataloader : DataLoader ‚Äî training data.\n",
    "        device : torch.device\n",
    "        mask_rate : float ‚Äî fraction of nodes supervised per graph.\n",
    "\n",
    "    Returns:\n",
    "        mean_loss : float ‚Äî mean training loss across batches.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    mean_loss = 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        # Generate dynamic supervision mask BEFORE forward pass\n",
    "        mask = model.get_mask(batch.num_nodes, rate=mask_rate, device=device)\n",
    "\n",
    "        # Forward pass using masked input\n",
    "        out = model(batch.pos, batch.signal, batch.edge_index, batch.edge_attr, mask)\n",
    "\n",
    "        # Compute loss only on supervised nodes\n",
    "        loss = criterion(out[mask], batch.signal[mask])\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        mean_loss += loss.item()\n",
    "\n",
    "    return mean_loss / len(dataloader)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validation(dataloader, device, mask_rate=0.1):\n",
    "    \"\"\"\n",
    "    Validate the model with dynamic masking per batch.\n",
    "\n",
    "    Args:\n",
    "        dataloader : DataLoader ‚Äî validation data.\n",
    "        device : torch.device\n",
    "        mask_rate : float ‚Äî fraction of nodes supervised per graph.\n",
    "\n",
    "    Returns:\n",
    "        mean_loss : float ‚Äî mean validation loss across batches.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    mean_loss = 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        # üî• Generate dynamic supervision mask BEFORE forward pass\n",
    "        mask = model.get_mask(batch.num_nodes, rate=mask_rate, device=device)\n",
    "\n",
    "        # Forward pass using masked input\n",
    "        out = model(batch.pos, batch.signal, batch.edge_index, batch.edge_attr, mask)\n",
    "\n",
    "        # Compute loss only on supervised nodes\n",
    "        loss = criterion(out[mask], batch.signal[mask])\n",
    "\n",
    "        mean_loss += loss.item()\n",
    "\n",
    "    return mean_loss / len(dataloader)"
   ],
   "id": "eeb0c5dd04cd65f4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### ‚öôÔ∏è Training Setup\n",
    "\n",
    "We now configure the components needed to train the model:\n",
    "\n",
    "- **Device Selection**  \n",
    "  Automatically use GPU (`cuda:0`) if available; otherwise fall back to CPU.\n",
    "\n",
    "- **Model Instantiation**  \n",
    "  Initialize the `BasicNet` model with:\n",
    "  - Positional feature size (`data.pos.shape[1]`)\n",
    "  - Signal feature size (`data.signal.shape[1]`)\n",
    "  - Hidden channels set to 64.\n",
    "\n",
    "- **Loss Function**  \n",
    "  We use **Mean Squared Error (MSELoss)** because the task is a regression problem (predicting continuous sine waves).\n",
    "\n",
    "- **Optimizer**  \n",
    "  **Adam optimizer** is used for its efficiency and robustness in training deep models.\n",
    "\n",
    "- **Early Stopping**  \n",
    "  An `EarlyStopper` is set up with:\n",
    "  - `patience=10` (number of epochs without improvement before stopping)\n",
    "  - `min_delta=0.0` (smallest improvement to be considered progress)\n",
    "\n",
    "- **Training State Variables**  \n",
    "  - `loss_train` and `loss_val` store the loss history.\n",
    "  - `best_loss` tracks the best validation loss seen so far.\n",
    "  - `PATH_CHECKPOINT` defines where to save the best model weights.\n",
    "\n",
    "---\n"
   ],
   "id": "1feac424d1dd1b89"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "\n",
    "\n",
    "model = BasicNet(data.pos.shape[1], data.signal.shape[1], hidden_channels=128).to(device)\n",
    "\n",
    "# criterion = torch.nn.MSELoss()  # Define loss criterion.\n",
    "criterion = torch.nn.SmoothL1Loss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  # Define optimizer.\n",
    "early_stopper = EarlyStopper(patience=10, min_delta=0.0)\n",
    "loss_train = []\n",
    "loss_val = []\n",
    "best_loss = 1\n",
    "best_epoch = 0\n",
    "PATH_CHECKPOINT = \"./checkpoint/best_masked.pt\"\n",
    "mask_rate = 0.2\n",
    "model"
   ],
   "id": "fa632c85bf99947f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### üîÅ Training Loop\n",
    "\n",
    "We now launch the model training:\n",
    "\n",
    "- Loop over a maximum of **5000 epochs**.\n",
    "- For each epoch:\n",
    "  1. Perform a full pass over the training data and compute the mean training loss.\n",
    "  2. Perform a full pass over the validation data and compute the mean validation loss.\n",
    "  3. Update the training progress bar with the latest losses.\n",
    "  4. If the validation loss improves, save the model checkpoint.\n",
    "  5. If early stopping criteria are met, stop training early.\n",
    "\n",
    "### üìà Monitoring:\n",
    "- We use `tqdm` to display a live progress bar with the latest training and validation loss.\n",
    "- The **best model** (lowest validation loss) is saved automatically to `PATH_CHECKPOINT`.\n",
    "\n",
    "> üß† **Reminder:**  \n",
    "> Dynamic masking is used both during training and validation, so different nodes are randomly supervised in every batch.\n",
    "\n",
    "### üß† Input Masking Strategy: Preventing Cheating\n",
    "\n",
    "Unlike typical label masking or partial supervision,  \n",
    "**we use masking to deliberately corrupt node inputs** during training.\n",
    "\n",
    "- For each training node, its **own input signal is zeroed out**.\n",
    "- The model **cannot access its own signal** directly.\n",
    "- It must **reconstruct the missing signal** using **only its neighbors** via message passing.\n",
    "\n",
    "‚úÖ All nodes still participate fully in the graph structure.  \n",
    "‚úÖ Only the input features of supervised nodes are masked.\n",
    "\n",
    "---\n",
    "\n",
    "**Training flow:**\n",
    "1. Full graph is built.\n",
    "2. Input signal of supervised nodes is zeroed out.\n",
    "3. Message passing happens across the entire graph.\n",
    "4. Loss is computed **only** at the masked (supervised) nodes.\n",
    "5. Gradients update the model to better reconstruct missing signals.\n",
    "\n",
    "---\n",
    "\n",
    "> üß† **This approach forces the GNN to learn meaningful spatial patterns**,  \n",
    "> and mimics real-world situations like seismic networks, GNSS, or sensor fields,  \n",
    "> where some stations may be missing data and need to interpolate intelligently.\n",
    "\n",
    "---"
   ],
   "id": "40f3250cfeaeef9e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "start_time = time.perf_counter()\n",
    "\n",
    "nb_epoch = tqdm(range(5000))\n",
    "\n",
    "for epoch in nb_epoch:\n",
    "    loss_train.append(train(train_loader, device, mask_rate))\n",
    "    loss_val.append(validation(val_loader, device, mask_rate))\n",
    "    \n",
    "    if loss_val[-1] < best_loss:\n",
    "        best_loss = loss_val[-1]\n",
    "        best_epoch = epoch\n",
    "        torch.save(model.state_dict(), PATH_CHECKPOINT)\n",
    "    if early_stopper.early_stop(loss_val[-1]):\n",
    "        print(f\"early stopping at epoch {epoch}: train loss={loss_train[-1]}, val loss={loss_val[-1]}\")\n",
    "        break\n",
    "    nb_epoch.set_postfix_str(f\"train loss={loss_train[-1]}, val loss={loss_val[-1]}\")"
   ],
   "id": "10d441a6b2e1d334"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plt.plot(loss_train, label=\"Train\")\n",
    "plt.plot(loss_val, label = \"Validation\")\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))"
   ],
   "id": "7be4c6e08df32cb1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## üß™ Model Evaluation\n",
    "\n",
    "After training, we evaluate the model‚Äôs performance on the test set.\n",
    "\n",
    "### üéØ Evaluation Goals:\n",
    "- Measure how well the model can **predict the full sine wave at each node**.\n",
    "- Assess overall performance with **quantitative metrics** (like Mean Absolute Error).\n",
    "- **Visualize** individual predictions to qualitatively inspect model behavior.\n",
    "\n",
    "### üö¶ Key Differences from Training:\n",
    "- **No dynamic masking during evaluation**:  \n",
    "  - We want to predict the full signal at every node, not just a subset.\n",
    "  - A mask of all `False` values is passed to the model to disable input corruption.\n",
    "- **Full supervision**:  \n",
    "  - The loss and metrics are computed across **all nodes** without random sampling.\n",
    "\n",
    "### üìä Evaluation Metrics:\n",
    "- **Mean Absolute Error (MAE)**:  \n",
    "  Measures the average absolute difference between predicted and true signals at each node.\n",
    "- **Histogram of per-node errors**:  \n",
    "  Helps understand the distribution of prediction errors across the entire test set.\n",
    "- **Qualitative plots**:  \n",
    "  Overlay predicted and true sine waves for random sample nodes to visually inspect performance.\n",
    "\n",
    "> üß† **Reminder:**  \n",
    "> Even though the model was trained with incomplete (masked) signals, during evaluation we test its ability to recover the entire signal at every node.\n",
    "\n",
    "---"
   ],
   "id": "b17707c2afe048f7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# restore best model\n",
    "PATH_CHECKPOINT = \"./checkpoint/best_masked.pt\"\n",
    "model.load_state_dict(torch.load(PATH_CHECKPOINT, map_location=torch.device('cpu')))\n",
    "print(f\"best loss={best_loss}, model eval loss={validation(val_loader, device, mask_rate)} at epoch {best_epoch}\")"
   ],
   "id": "8340c83d56c0468c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### üõ†Ô∏è Evaluation Function\n",
    "\n",
    "We define an `evaluate()` function to test the model‚Äôs performance on the full test set.\n",
    "\n",
    "Key differences compared to training:\n",
    "- **No dynamic masking**:  \n",
    "  We pass a \"no mask\" (all `False`) tensor to the model. This ensures **full input signals** are used for prediction.\n",
    "- **Full node supervision**:  \n",
    "  The loss and metrics are computed across **all nodes**, not just a random subset.\n",
    "\n",
    "### üìã Function Behavior:\n",
    "- Perform a forward pass through the model for each batch in the test set.\n",
    "- Compute the **Mean Absolute Error (MAE)** for each batch.\n",
    "- Stack all predicted signals and ground truth signals for further visualization.\n",
    "\n",
    "### üì§ Function Outputs:\n",
    "- **`mean_mae`**: Average Mean Absolute Error across all nodes in the test set.\n",
    "- **`pred_all`**: Stacked predictions for all nodes.\n",
    "- **`true_all`**: Stacked ground truth sine waves for all nodes.\n",
    "\n",
    "> üß† **Reminder:**  \n",
    "> Evaluating on full signals allows us to truly assess how well the model reconstructs complete sine waves at each node.\n"
   ],
   "id": "91dcf962477e1960"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, dataloader, device):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the full test set without dynamic masking.\n",
    "\n",
    "    Args:\n",
    "        model : Trained model\n",
    "        dataloader : DataLoader for the test set\n",
    "        device : torch.device\n",
    "\n",
    "    Returns:\n",
    "        mean_mae : float ‚Äî Mean Absolute Error across all nodes\n",
    "        pred_all : Tensor ‚Äî Predicted signals (stacked) [num_nodes, signal_size]\n",
    "        true_all : Tensor ‚Äî Ground truth signals (stacked) [num_nodes, signal_size]\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_mae = 0\n",
    "    pred_all = []\n",
    "    true_all = []\n",
    "\n",
    "    for batch in dataloader:\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        # Create a zero mask ‚Üí No input corruption during final evaluation\n",
    "        no_mask = torch.zeros(batch.num_nodes, dtype=torch.bool, device=device)\n",
    "\n",
    "        # Forward pass\n",
    "        out = model(batch.pos, batch.signal, batch.edge_index, batch.edge_weight, no_mask)\n",
    "\n",
    "        # Accumulate outputs\n",
    "        pred_all.append(out.cpu())\n",
    "        true_all.append(batch.signal.cpu())\n",
    "\n",
    "        # Calculate MAE for the batch\n",
    "        batch_mae = torch.mean(torch.abs(out - batch.signal))\n",
    "        total_mae += batch_mae.item()\n",
    "\n",
    "    # Aggregate\n",
    "    mean_mae = total_mae / len(dataloader)\n",
    "    pred_all = torch.cat(pred_all, dim=0)\n",
    "    true_all = torch.cat(true_all, dim=0)\n",
    "\n",
    "    return mean_mae, pred_all, true_all\n"
   ],
   "id": "7451181724a55bf4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### üîç Prediction and Visualization\n",
    "\n",
    "After evaluating the model numerically, it is crucial to **visualize** how well it is reconstructing the node-level sine waves.\n",
    "\n",
    "We perform two types of visual inspections:\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ 1. Random Node Signal Comparison\n",
    "\n",
    "- Randomly select a few nodes from the test set.\n",
    "- Plot both the **true sine wave** and the **predicted sine wave** for each node.\n",
    "- This helps us visually inspect how well the model captures the correct waveform shapes.\n",
    "\n",
    "> üß† **Note:**  \n",
    "> Perfect predictions would show overlapping dashed (true) and solid (predicted) lines.\n",
    "\n",
    "---\n",
    "\n",
    "### üìä 2. Histogram of Node Prediction Errors\n",
    "\n",
    "- Compute the **L2 norm** (Euclidean distance) between the predicted and true signals at each node.\n",
    "- Plot a histogram showing the distribution of node-wise errors across the entire test set.\n",
    "\n",
    "This helps answer questions like:\n",
    "- Are most predictions very close?\n",
    "- Are there occasional large errors (outliers)?\n",
    "\n",
    "> üö¶ **Goal:**  \n",
    "> Ideally, the histogram is **skewed toward small errors**, indicating most nodes are well predicted!\n",
    "\n",
    "---\n"
   ],
   "id": "e0ef61fea0aa0d74"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# --- Predict and gather outputs ---\n",
    "mean_mae, pred_all, true_all = evaluate(model, test_loader, device)\n",
    "\n",
    "print(f\"Mean Absolute Error (MAE) on Test Set: {mean_mae:.4f}\")"
   ],
   "id": "2a61b68045a8534"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# --- Compute per-node L2 errors ---\n",
    "node_errors = torch.norm(pred_all - true_all, dim=1).numpy()\n",
    "\n",
    "# --- Estimate random prediction baseline ---\n",
    "# Random predictions between -1 and 1 (since tanh activation output is bounded)\n",
    "rand_preds = 2 * torch.rand_like(true_all) - 1  # Uniform random between [-1, 1]\n",
    "rand_errors = torch.norm(rand_preds - true_all, dim=1).numpy()\n",
    "expected_random_error = rand_errors.mean()\n",
    "\n",
    "pred_0 = torch.zeros(true_all.shape)\n",
    "pred_0_errors = torch.norm(pred_0 - true_all, dim=1).numpy()\n",
    "expected_pred_0_error = pred_0_errors.mean()\n",
    "\n",
    "# --- Plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(node_errors, bins=30, alpha=0.7, color='teal', edgecolor='black', label=\"Model Prediction Errors\")\n",
    "plt.axvline(expected_random_error, color=\"red\", linestyle=\"--\", linewidth=2, label=f\"Random baseline: {expected_random_error:.2f}\")\n",
    "plt.axvline(expected_pred_0_error, color=\"black\", linestyle=\"--\", linewidth=2, label=f\"0 baseline: {expected_pred_0_error:.2f}\")\n",
    "\n",
    "plt.xlabel(\"L2 Error per Node\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Histogram of Node Prediction Errors\")\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "90b1927be7476b29"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# --- Pick random nodes ---\n",
    "num_samples = 9\n",
    "random_indices = random.sample(range(pred_all.shape[0]), num_samples)\n",
    "\n",
    "fig, axs = plt.subplots(3, 3, figsize=(15, 8))\n",
    "axs = axs.flatten()\n",
    "\n",
    "for i, idx in enumerate(random_indices):\n",
    "    axs[i].plot(true_all[idx].numpy(), label=\"True Signal\", linestyle='--')\n",
    "    axs[i].plot(pred_all[idx].numpy(), label=\"Predicted Signal\", linestyle='-')\n",
    "    axs[i].set_title(f\"Node {idx}\")\n",
    "    axs[i].legend()\n",
    "    axs[i].grid(True, linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "plt.suptitle(\"Predicted vs True Sine Waves at Random Nodes\", fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()"
   ],
   "id": "eb544347c03119ae"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### üåê Spatial Visualization of Node Errors\n",
    "\n",
    "To better understand where the model performs well (or poorly),  \n",
    "we visualize the spatial distribution of node-level errors across multiple random test graphs.\n",
    "\n",
    "For each selected graph:\n",
    "- **Nodes** are colored according to their **Mean Absolute Error (MAE)** between predicted and true signals.\n",
    "- **Edges** are shown in light gray to illustrate graph connectivity.\n",
    "- A colorbar indicates the magnitude of error at each node.\n",
    "\n",
    "### üß† Interpretation:\n",
    "- Lower-error nodes (dark blue) suggest the model can accurately reconstruct the signal.\n",
    "- Higher-error nodes (yellow/green) highlight more challenging areas.\n",
    "- By comparing graphs, we can investigate if node **density** and **connectivity** correlate with prediction quality.\n",
    "\n",
    "> üîç **Hypothesis:**  \n",
    "> Nodes in densely connected areas should have lower errors, as the model can better leverage neighbor information.\n",
    "\n",
    "---\n"
   ],
   "id": "79798de6cf5e7c22"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# --- Sample 9 graphs randomly ---\n",
    "sample_graph_indices = random.sample(range(len(test_dataset)), 9)\n",
    "\n",
    "fig, axs = plt.subplots(3, 3, figsize=(18, 14))\n",
    "axs = axs.flatten()\n",
    "\n",
    "node_pointer = 0\n",
    "\n",
    "for i, graph_idx in enumerate(sample_graph_indices):\n",
    "    g = test_dataset[graph_idx]\n",
    "    num_nodes = g.pos.shape[0]\n",
    "\n",
    "    # Extract predictions and truths for this graph\n",
    "    preds = pred_all[node_pointer:node_pointer + num_nodes]\n",
    "    trues = true_all[node_pointer:node_pointer + num_nodes]\n",
    "\n",
    "    # Compute per-node MAE\n",
    "    node_errors = torch.mean(torch.abs(preds - trues), dim=1).numpy()\n",
    "\n",
    "    # --- Use the new helper to plot into specific axis\n",
    "    visualize_graph_torch(\n",
    "        g,\n",
    "        node_values=node_errors,\n",
    "        cmap=\"plasma\",\n",
    "        ax=axs[i],\n",
    "        title=f\"Graph {graph_idx} ‚Äî Node Errors\"\n",
    "    )\n",
    "\n",
    "    node_pointer += num_nodes\n",
    "\n",
    "# Hide any unused subplots (shouldn't happen since we picked exactly 9)\n",
    "for j in range(len(sample_graph_indices), len(axs)):\n",
    "    axs[j].axis('off')\n",
    "\n",
    "plt.suptitle(\"Node MAE Errors Across Random Test Graphs\", fontsize=20)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()\n"
   ],
   "id": "b581a55f912454df"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## üèÅ Conclusion\n",
    "\n",
    "In this notebook, we built and trained a Graph Neural Network (GNN) model for **node-level sine wave reconstruction** under incomplete input conditions.  \n",
    "We covered the full learning workflow:\n",
    "\n",
    "- üõ†Ô∏è Built a synthetic graph dataset with delayed sine wave signals based on spatial distance from a hidden origin.\n",
    "- üß† Designed a custom GNN architecture (`BasicNet`) combining:\n",
    "  - Direct fusion of spatial coordinates and signal inputs\n",
    "  - Dual GATv2Conv layers for multi-hop attention-based message passing\n",
    "  - Edge features based on 2D relative position vectors `(dx, dy)`\n",
    "- üéØ Trained the model using **dynamic input masking** to prevent nodes from using their own signals during reconstruction.\n",
    "- üìà Monitored training progress with early stopping and validation loss tracking.\n",
    "- üß™ Evaluated model performance using:\n",
    "  - Mean Absolute Error (MAE)\n",
    "  - Full signal predictions vs. true signals\n",
    "  - Error histograms compared to random baselines\n",
    "  - Spatial visualization of prediction errors across different graphs\n",
    "\n",
    "---\n",
    "\n",
    "### üöÄ Key Takeaways:\n",
    "- GNNs can successfully **reconstruct masked signals** by leveraging graph connectivity and neighborhood aggregation.\n",
    "- Using directional edge features `(dx, dy)` improves the model's spatial awareness and prediction quality.\n",
    "- Dense graphs and sufficient neighborhood information help achieve better signal recovery.\n",
    "- Dynamic input masking simulates real-world challenges like missing or corrupted sensor data.\n",
    "\n",
    "---\n",
    "\n",
    "### üîÆ Possible Extensions:\n",
    "- Stack additional GATv2Conv layers to improve long-range message passing.\n",
    "- Experiment with different numbers of attention heads or alternative pooling strategies.\n",
    "- Test different masking strategies (e.g., structured masking or temporal gaps).\n",
    "- Apply the same framework to real geophysical sensor networks or temporal data prediction tasks.\n",
    "\n",
    "---\n",
    "\n",
    "### üìö Want to see GNNs applied to real geophysical datasets?\n",
    "\n",
    "If you're interested in how Graph Neural Networks can extend beyond synthetic examples,  \n",
    "check out our recent research applying GNNs to **denoise real daily GNSS time series** in the Cascadia subduction zone:\n",
    "\n",
    "> **Bachelot, L., Thomas, A. M., Melgar, D., Searcy, J., & Sun, Y.-S. (2025).**  \n",
    "> *Cascadia Daily GNSS Time Series Denoising: Graph Neural Network and Stack Filtering.*  \n",
    "> Seismica, 2(4). [https://doi.org/10.26443/seismica.v2i4.1419](https://doi.org/10.26443/seismica.v2i4.1419)\n",
    "\n",
    "---"
   ],
   "id": "ecf957ce32297cc4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "626a7b1e140ad07f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pangeo_torch]",
   "language": "python",
   "name": "conda-env-pangeo_torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
